{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/ml_and_nlp/blob/main/1-Pre-NLP/3_Text_Processing_Logistic_Regression_and_Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBT0kSX6UkRJ"
      },
      "source": [
        "# Logistic Regression and Boosting Algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1ibhKLUkRW"
      },
      "source": [
        "<a id=\"predicting-a-categorical-response\"></a>\n",
        "## Predicting a Single Categorical Response\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing stuff"
      ],
      "metadata": {
        "id": "KSob7NpKVP3i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipxwHoE9HUCF"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade textblob spacy 'gensim==4.2.0' swifter keras_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmP43_eVHUCG"
      },
      "outputs": [],
      "source": [
        "!python -m textblob.download_corpora lite\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h80K7JvGHUCJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from textblob import TextBlob, Word\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import spacy\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "textblob_tokenizer = lambda x: TextBlob(x).words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f yelp.csv ]; then\n",
        "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
        "fi"
      ],
      "metadata": {
        "id": "k6RoBSKKLvJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash get_data.sh"
      ],
      "metadata": {
        "id": "OcmMkgZqLxqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zG2McSWHUCK"
      },
      "outputs": [],
      "source": [
        "# Read yelp.csv into a DataFrame.\n",
        "path = './yelp.csv'\n",
        "yelp = pd.read_csv(path)\n",
        "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
        "yelp_best_worst = yelp[ (yelp.stars == 1) | (yelp.stars == 5) ]\n",
        "\n",
        "# Define X and y.\n",
        "X = yelp_best_worst.text\n",
        "y = yelp_best_worst.stars\n",
        "\n",
        "# Split the new DataFrame into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUyePxrQUkRe"
      },
      "source": [
        "<a id=\"using-logistic-regression-for-classification\"></a>\n",
        "## Using Logistic Regression for Classification\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGa-USNjUkRe"
      },
      "outputs": [],
      "source": [
        "# Fit a logistic regression model and store the class predictions.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "logreg.fit(X,y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course this simply fails, we need to preprocess the text, convert it into a Tensor format and then and only then we can use models!"
      ],
      "metadata": {
        "id": "_VwFV23JVpwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting text to vectors"
      ],
      "metadata": {
        "id": "jGrSbpCXVzMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "nltk.download('stopwords')\n",
        "my_stopwords = nltk.corpus.stopwords.words('english')\n",
        "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
        "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
        "\n",
        "\n",
        "def preprocess_text(text, should_join=True):\n",
        "    text = ' '.join(word.lower() for word in textblob_tokenizer(text))\n",
        "    text = re.sub(r'http\\S+', '', text) # remove http links\n",
        "    text = re.sub(r'bit.ly/\\S+', '', text) # rempve bitly links\n",
        "    text = text.strip('[link]') # remove [links]\n",
        "    text = re.sub('['+my_punctuation + ']+', ' ', text) # remove punctuation\n",
        "    text = re.sub('\\s+', ' ', text) #remove double spacing\n",
        "    text = re.sub(r\"[^a-zA-Z.,&!?]+\", r\" \", text) # only normal characters\n",
        "    text_token_list = [word for word in text.split(' ')\n",
        "                            if word not in my_stopwords] # remove stopwords\n",
        "    text_token_list = [word_rooter(word) if '#' not in word else word\n",
        "                        for word in text_token_list] # apply word rooter\n",
        "    text = ' '.join(text_token_list)\n",
        "    if should_join:\n",
        "      return ' '.join(gensim.utils.simple_preprocess(text))\n",
        "    else:\n",
        "      return gensim.utils.simple_preprocess(text)"
      ],
      "metadata": {
        "id": "qMVNuEtcVoym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import swifter\n",
        "X_preprocessed = X.swifter.apply(preprocess_text)"
      ],
      "metadata": {
        "id": "0kgWYlPpVo1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_preprocessed[0]"
      ],
      "metadata": {
        "id": "CwMeNt_sVo4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we pass from text to numbers? With tokenizers. We will use Tensorflow ones!"
      ],
      "metadata": {
        "id": "mQH3a91nWm3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find a set named vocab that has all unique words\n"
      ],
      "metadata": {
        "id": "2N2Y_sleVo7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{len(vocab)} unique words')"
      ],
      "metadata": {
        "id": "grvROE8IVpA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_maximum_review_length(srs):\n",
        "    pass\n",
        "    return maximum\n",
        "\n",
        "\n",
        "maximum = get_maximum_review_length(X_preprocessed)"
      ],
      "metadata": {
        "id": "9n5_6tl-VpDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The maximum review was {maximum} words long')"
      ],
      "metadata": {
        "id": "3sRVrTUhVpGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "ids_from_words = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "bjiUQ6h1VpIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_from_ids = preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_words.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "HKuB1UDiVpLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(words_from_ids(ids), axis=-1, separator=' ')"
      ],
      "metadata": {
        "id": "ME_wWwsvVpOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_words(preprocess_text('Only you can prevent forest fires', should_join=False))\n",
        "ids"
      ],
      "metadata": {
        "id": "Op37f7EjVpQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text('Only you can prevent forest fires', should_join=False)"
      ],
      "metadata": {
        "id": "712kFWwb1dqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_from_ids(ids)\n"
      ],
      "metadata": {
        "id": "VHSIBP6EVpS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequence_of_tokens(x, maxlen, unk_token='[UNK]'):\n",
        "  if len(x)<maxlen:\n",
        "    x.extend([unk_token]*(maxlen-len(x)))\n",
        "  return x"
      ],
      "metadata": {
        "id": "LpCZqSlQYb2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQTtWBrFYpn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "# Very useful method to keep in mind\n",
        "def get_ids_tensor(srs):\n",
        "\n",
        "  processed = srs.swifter.apply(lambda x: pad_sequence_of_tokens(preprocess_text(x, should_join=False), maxlen=maximum)).to_list()\n",
        "  return tf.squeeze(tf.constant(pad_sequences(ids_from_words(processed), maxlen=maximum, padding='post'), dtype='int32'))\n",
        "\n"
      ],
      "metadata": {
        "id": "ApOkaOuFYb5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = get_ids_tensor(srs=X_preprocessed.reset_index(drop=True))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "nyZpEFMGYb_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids.shape"
      ],
      "metadata": {
        "id": "-Oprft54YcDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(all_ids.numpy(), y, test_size=0.25 ,random_state=99)"
      ],
      "metadata": {
        "id": "9FwE3RhcYcLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Logistic Regression"
      ],
      "metadata": {
        "id": "LRq1lAuCaOSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train a Logistic Regression on X_train and give the accuracy\n",
        "logreg = None\n"
      ],
      "metadata": {
        "id": "pg_YCHXmaRQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "y_pred = logreg.predict(X_test)\n",
        "print((metrics.accuracy_score(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "8f-LC-qRaqN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5sk57IRUkRh"
      },
      "source": [
        "<a id=\"probability-odds-e-log-and-log-odds\"></a>\n",
        "### Probability, e, Log, and Log Odds\n",
        "---\n",
        "\n",
        "To understand how logistic regression predicts the probability of class membership we need to start by understanding the relationship between probability, odds ratios, and log odds ratios. This is because logistic regression predicts log odds and so reading log odds is extremely useful for interpreting logistic regression.\n",
        "\n",
        "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n",
        "\n",
        "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n",
        "\n",
        "It is often useful to think of the numeric odds as a ratio. For example, 5/1 = 5 odds is \"5 to 1\" -- five wins for every one loss (e.g. of six total plays). 2/3 odds means \"2 to 3\" -- two wins for every three losses (e.g. of five total plays).\n",
        "\n",
        "Examples:\n",
        "\n",
        "- Dice roll of 1: probability = 1/6, odds = 1/5\n",
        "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n",
        "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n",
        "\n",
        "$$odds = \\frac {probability} {1 - probability}$$\n",
        "\n",
        "$$probability = \\frac {odds} {1 + odds}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAYEEBUfUkRi"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-IPqBINUkRi"
      },
      "source": [
        "<a id=\"understanding-e-and-the-natural-logarithm\"></a>\n",
        "### Understanding e and the Natural Logarithm\n",
        "\n",
        "What is e? It is the base rate of growth shared by all continually growing processes:\n",
        "\n",
        "e is the irrational base of the natural log `ln`.\n",
        "\n",
        "- 2.718281828459"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHPdharEUkRi"
      },
      "outputs": [],
      "source": [
        "# Exponential function: e^1\n",
        "e = np.exp(1)\n",
        "e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bCa8GO9UkRi"
      },
      "source": [
        "What is a (natural) log? It gives you the time needed to reach a certain level of growth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dchcn5-dUkRi"
      },
      "outputs": [],
      "source": [
        "# Time needed to grow 1 unit to 2.718 units\n",
        "# ln e = 1\n",
        "np.log(2.718281828459) # very close to previous value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBZH-R1zUkRi"
      },
      "outputs": [],
      "source": [
        "np.log(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_CRRD4WUkRj"
      },
      "source": [
        "It is also the inverse of the exponential function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_AMiSqWUkRj"
      },
      "outputs": [],
      "source": [
        "# e^5\n",
        "np.exp(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jBourO9UkRj"
      },
      "outputs": [],
      "source": [
        "# np.exp(5)\n",
        "2.7182818**5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa7P7USzUkRj"
      },
      "outputs": [],
      "source": [
        "# Taking the log of the exponential returns back to original input\n",
        "np.log(np.exp(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liCrIG1NUkRk"
      },
      "source": [
        "Lets take one of our odds from out table and walk through how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6f9xknAUkRk"
      },
      "outputs": [],
      "source": [
        "# Odds = 0.25\n",
        "# ln 0.25 = -1.38629436\n",
        "np.log(0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev4qLd1wUkRk"
      },
      "outputs": [],
      "source": [
        "print(e**-1.3862943611198906)\n",
        "print(np.exp(-1.3862943611198906))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpsQ8yl-UkRk"
      },
      "source": [
        "<img src=\"https://www.dropbox.com/scl/fi/3s5btre5ofls0efzbmj93/log_vs_ols.jpg?rlkey=bc6wf7ejoygrdbk1bqqsn6851&raw=1\"  align=\"Center\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-oyHgSKUkRk"
      },
      "source": [
        "<a id=\"what-is-logistic-regression\"></a>\n",
        "## What Is Logistic Regression?\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du_wEuzxUkRl"
      },
      "source": [
        "Linear regression: Continuous response is modeled as a linear combination of the features.\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1x$$\n",
        "\n",
        "Logistic regression: Log odds of a categorical response being \"true\" (1) is modeled as a linear combination of the features.\n",
        "\n",
        "$$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$\n",
        "\n",
        "This is called the logit function.\n",
        "\n",
        "Probability is sometimes written as pi.\n",
        "\n",
        "$$\\log \\left({\\pi\\over 1-\\pi}\\right) = \\beta_0 + \\beta_1x$$\n",
        "\n",
        "The equation can be rearranged into the logistic function.\n",
        "\n",
        "$$\\hat{p} = \\frac{e^{\\beta_0 + \\beta_1x}} {1 + e^{\\beta_0 + \\beta_1x}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixHO2DUXUkRl"
      },
      "source": [
        "In other words:\n",
        "\n",
        "- Logistic regression outputs the probabilities of a specific class.\n",
        "- Those probabilities can be converted into class predictions.\n",
        "\n",
        "The logistic function has some nice properties:\n",
        "\n",
        "- Takes on an \"s\" shape\n",
        "- Output is bounded by 0 and 1\n",
        "\n",
        "We have covered how this works for binary classification problems (two response classes). But what about multi-class classification problems (more than two response classes)?\n",
        "\n",
        "- The most common solution for classification models is \"one-vs-all\" (also known as \"one-vs-rest\"): Decompose the problem into multiple binary classification problems.\n",
        "- Multinomial logistic regression, on the other hand, can solve this as a single problem, but how this works is beyond the scope of this lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTxKGUwUUkRq"
      },
      "source": [
        "<a id=\"comparing-logistic-regression-to-other-models\"></a>\n",
        "## Comparing Logistic Regression to Other Models\n",
        "---\n",
        "\n",
        "Advantages of logistic regression:\n",
        "\n",
        "- Highly interpretable (if you remember how).\n",
        "- Model training and prediction are fast.\n",
        "- No tuning is required (excluding regularization).\n",
        "- Features don't need scaling.\n",
        "- Can perform well with a small number of observations.\n",
        "- Outputs well-calibrated predicted probabilities.\n",
        "\n",
        "Disadvantages of logistic regression:\n",
        "\n",
        "- Presumes a linear relationship between the features and the log odds of the response.\n",
        "- Performance is (generally) not competitive with the best supervised learning methods.\n",
        "- Can't automatically learn feature interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_2bFtOmUkRr"
      },
      "source": [
        "----\n",
        "\n",
        "## Is accuracy is good metric?\n",
        "<img src=\"https://www.dropbox.com/scl/fi/ppmtzavh3ot6jinkb35vd/accuracy.png?rlkey=32gfvubdwn1p4i9anwut7ug39&raw=1\"  align=\"center\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYM2D6T1UkRr"
      },
      "source": [
        "### The Accuracy Paradox\n",
        "\n",
        "Accuracy is a very intuitive metric — it's a lot like an exam score where you get total correct/total attempted. However, accuracy is often a poor metric in application. There are many reasons for this:\n",
        "- Imbalanced problems problems with 95% positives in the baseline will have 95% accuracy even with no predictive power.\n",
        "  - This is the paradox; pursuing accuracy often means predicting the most common class rather than doing the most useful work.\n",
        "- Applications often have uneven penalties and rewards for true positives and false positives.\n",
        "- Ranking predictions in the correct order be more important than getting them correct.\n",
        "- In many case we need to know the exact probability of a positives and negatives.\n",
        "  - To calculate an expected return.\n",
        "  - To triage observations that are borderline positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umb7u9dtUkRs"
      },
      "source": [
        "<a id=\"advanced-classification-metrics\"></a>\n",
        "## Advanced Classification Metrics\n",
        "\n",
        "---\n",
        "\n",
        "When we evaluate the performance of a logistic regression (or any classifier model), the standard metric to use is accuracy: How many class labels did we guess correctly? However, accuracy is only one of several metrics we could use when evaluating a classification model.\n",
        "\n",
        "$$Accuracy = \\frac{total~predicted~correct}{total~predicted}$$\n",
        "\n",
        "Accuracy alone doesn’t always give us a full picture.\n",
        "\n",
        "If we know a model is 75% accurate, it doesn’t provide any insight into why the 25% was wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T95QzheUkRs"
      },
      "source": [
        "Consider a binary classification problem where we have 165 observations/rows of people who are either smokers or nonsmokers.\n",
        "\n",
        "<table style=\"border: none\">\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
        "    <td style=\"\"><b>Predicted: No</b></td>\n",
        "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: No</b></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: Yes</b></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "</tr>\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GISjnWvbUkRs"
      },
      "source": [
        "There are 60 in class 0, nonsmokers, and 105 observations in class 1, smokers\n",
        "<table style=\"border: none\">\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
        "    <td style=\"\"><b>Predicted: No</b></td>\n",
        "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: No</b></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\">60</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: Yes</b></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\">105</td>\n",
        "</tr>\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qONng8RpUkRs"
      },
      "source": [
        "We have 55 predictions of class, predicted as nonsmokers, and 110 of class 1, predicted to be smokers.\n",
        "\n",
        "<table style=\"border: none\">\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
        "    <td style=\"\"><b>Predicted: No</b></td>\n",
        "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: No</b></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\">60</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: Yes</b></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\"></td>\n",
        "    <td style=\"text-align: center\">105</td>\n",
        "</tr>\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none\"></td>\n",
        "    <td style=\"text-align: center\">55</td>\n",
        "    <td style=\"text-align: center\">110</td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHPv6L6BUkRs"
      },
      "source": [
        "- **True positives (TP):** These are cases in which we predicted yes (smokers), and they actually are smokers.\n",
        "- **True negatives (TN):** We predicted no, and they are nonsmokers.\n",
        "- **False positives (FP):** We predicted yes, but they were not actually smokers. (This is also known as a \"Type I error.\")\n",
        "- **False negatives (FN):** We predicted no, but they are smokers. (This is also known as a \"Type II error.\")\n",
        "<table style=\"border: none\">\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
        "    <td style=\"\"><b>Predicted: No</b></td>\n",
        "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: No</b></td>\n",
        "    <td style=\"text-align: center\">TN = 50</td>\n",
        "    <td style=\"text-align: center\">FP = 10</td>\n",
        "    <td style=\"text-align: center\">60</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: Yes</b></td>\n",
        "    <td style=\"text-align: center\">FN = 5</td>\n",
        "    <td style=\"text-align: center\">TP = 100</td>\n",
        "    <td style=\"text-align: center\">105</td>\n",
        "</tr>\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none\"></td>\n",
        "    <td style=\"text-align: center\">55</td>\n",
        "    <td style=\"text-align: center\">110</td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKhOMc-BUkRs"
      },
      "source": [
        "**Categorize these as TP, TN, FP, or FN:**\n",
        "\n",
        "Try not to look at the answers above.\n",
        "    \n",
        "- We predict nonsmoker, but the person is a smoker.\n",
        "- We predict nonsmoker, and the person is a nonsmoker.\n",
        "- We predict smoker and the person is a smoker.\n",
        "- We predict smoker and the person is a nonsmoker.\n",
        "\n",
        "<!--ANSWER\n",
        "- FN\n",
        "- TN\n",
        "- TP\n",
        "- FP\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEjVoGKUUkRt"
      },
      "source": [
        "<a id=\"accuracy-true-positive-rate-and-false-negative-rate\"></a>\n",
        "### Accuracy, True Positive Rate, and False Negative Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnOAx5OsUkRt"
      },
      "source": [
        "**Accuracy:** Overall, how often is the classifier correct?\n",
        "\n",
        "<span>\n",
        "    (<span style=\"color: green\">TP</span>+<span style=\"color: red\">TN</span>)/<span style=\"color: blue\">total</span> = (<span style=\"color: green\">100</span>+<span style=\"color: red\">50</span>)/<span style=\"color: blue\">165</span> = 0.91\n",
        "</span>\n",
        "\n",
        "<table style=\"border: none\">\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none; vertical-align: bottom; color: blue\">n = 165</td>\n",
        "    <td style=\"\"><b>Predicted: No</b></td>\n",
        "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: No</b></td>\n",
        "    <td style=\"text-align: center; background-color: red\">TN = 50</td>\n",
        "    <td style=\"text-align: center\">FP = 10</td>\n",
        "    <td style=\"text-align: center\">60</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: Yes</b></td>\n",
        "    <td style=\"text-align: center\">FN = 5</td>\n",
        "    <td style=\"text-align: center; background-color: green\">TP = 100</td>\n",
        "    <td style=\"text-align: center\">105</td>\n",
        "</tr>\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none\"></td>\n",
        "    <td style=\"text-align: center\">55</td>\n",
        "    <td style=\"text-align: center\">110</td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPgKEx7jUkRt"
      },
      "source": [
        "**True positive rate (TPR)** asks, “Out of all of the target class labels, how many were accurately predicted to belong to that class?”\n",
        "\n",
        "For example, given a medical exam that tests for cancer, how often does it correctly identify patients with cancer?\n",
        "\n",
        "<span>\n",
        "<span style=\"color: green\">TP</span>/<span style=\"color: blue\">actual yes</span> = <span style=\"color: green\">100</span>/<span style=\"color: blue\">105</span> = 0.95\n",
        "</span>\n",
        "\n",
        "<table style=\"border: none\">\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
        "    <td style=\"\"><b>Predicted: No</b></td>\n",
        "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: No</b></td>\n",
        "    <td style=\"text-align: center\">TN = 50</td>\n",
        "    <td style=\"text-align: center\">FP = 10</td>\n",
        "    <td style=\"text-align: center\">60</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: Yes</b></td>\n",
        "    <td style=\"text-align: center\">FN = 5</td>\n",
        "    <td style=\"text-align: center;background-color: green\">TP = 100</td>\n",
        "    <td style=\"text-align: center;color: blue\">105</td>\n",
        "</tr>\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none\"></td>\n",
        "    <td style=\"text-align: center\">55</td>\n",
        "    <td style=\"text-align: center\">110</td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWGwsnzDUkRt"
      },
      "source": [
        "**False positive rate (FPR)** asks, “Out of all items not belonging to a class label, how many were predicted as belonging to that target class label?”\n",
        "\n",
        "For example, given a medical exam that tests for cancer, how often does it trigger a “false alarm” by incorrectly saying a patient has cancer?\n",
        "\n",
        "<span>\n",
        "<span style=\"color: orange\">FP</span>/<span style=\"color: blue\">actual no</span> = <span style=\"color: orange\">10</span>/<span style=\"color: blue\">60</span> = 0.17\n",
        "</span>\n",
        "\n",
        "<table style=\"border: none\">\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
        "    <td style=\"\"><b>Predicted: No</b></td>\n",
        "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: No</b></td>\n",
        "    <td style=\"text-align: center\">TN = 50</td>\n",
        "    <td style=\"text-align: center;background-color: orange\">FP = 10</td>\n",
        "    <td style=\"text-align: center;color:blue\">60</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><b>Actual: Yes</b></td>\n",
        "    <td style=\"text-align: center\">FN = 5</td>\n",
        "    <td style=\"text-align: center\">TP = 100</td>\n",
        "    <td style=\"text-align: center\">105</td>\n",
        "</tr>\n",
        "<tr style=\"border: none\">\n",
        "    <td style=\"border: none\"></td>\n",
        "    <td style=\"text-align: center\">55</td>\n",
        "    <td style=\"text-align: center\">110</td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZgW8oFcUkRu"
      },
      "source": [
        "**Can you see that we might weigh TPR AND FPR differently depending on the situation?**\n",
        "\n",
        "- Give an example when we care about TPR, but not FPR.\n",
        "- Give an example when we care about FPR, but not TPR.\n",
        "\n",
        "<!--\n",
        "ANSWER:\n",
        "- During an initial medical diagnosis, we want to be sensitive. We want initial screens to come up with a lot of true positives, even if we get a lot of false positives.\n",
        "- If we are doing spam detection, we want to be precise. Anything that we remove from an inbox must be spam, which may mean accepting fewer true positives.\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7vTQeGgUkRu"
      },
      "source": [
        "## The Confusion Matrix\n",
        "<img src=\"https://www.dropbox.com/scl/fi/eaig0g63lct0r7dzt6qit/confusion.png?rlkey=edw4kjneymt6q0hl6r48512py&raw=1\" align=\"center\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8TwtY14UkRu"
      },
      "source": [
        "# What is precison and recall?\n",
        "<img src=\"https://www.dropbox.com/scl/fi/yf2rcxzstnz6p4ef11xj0/recall.png?rlkey=mmbfwbgvzadrwpgqwi44gxhnk&raw=1\" align=\"center\"/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "580e6cr3cM1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5IdYxD6UkRv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuT26991UkRw"
      },
      "source": [
        "## Lesson Review\n",
        "- **Logistic regression**\n",
        "  - What kind of machine learning problems does logistic regression address?\n",
        "  - What do the coefficients in a logistic regression represent? How does the interpretation differ from ordinary least squares? How is it similar?\n",
        "  \n",
        "- **The confusion matrix**\n",
        "  - How do true positive rate and false positive rate help explain accuracy?\n",
        "  - Why might one classification metric be more important to tune than another? Give an example of a business problem or project where this would be the case."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Boosting Algorithms and other things"
      ],
      "metadata": {
        "id": "sm--rNNocZm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "clf = GradientBoostingClassifier(n_estimators=50, learning_rate=0.5)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Mj26V5b0cmev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "clf = AdaBoostClassifier(n_estimators=50, learning_rate=0.5)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "jX7NK1QOc7o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q2Ufd7zrAfxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=50)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "SgI4faRIdKD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiclass Classification"
      ],
      "metadata": {
        "id": "KkCMvMmPhoPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just check in the estimators, most support multiclass classification."
      ],
      "metadata": {
        "id": "N79KIaamiOKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "X, y = load_iris(return_X_y=True)\n",
        "clf = LogisticRegression(random_state=0, multi_class='multinomial').fit(X, y)\n",
        "clf.predict(X[:2, :])\n",
        "clf.predict_proba(X[:2, :])\n",
        "clf.score(X, y)"
      ],
      "metadata": {
        "id": "bphOQQ2IhrHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Homework**: Try to perform the stars classification with Logistic Regression but without filtering only for 5 and 1 stars."
      ],
      "metadata": {
        "id": "Ad5QRe4TiUPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {
        "id": "MbwlFBil8ooj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we will use the [Iris](https://scikit-learn.org/stable/datasets/index.html#iris-dataset) open dataset and scikit-learn clustering algorithms to effectively return the species of all those flowers!\n",
        "\n",
        "The objective of this lab is to understand how clustering can help us identify \"potential\" labels of our dataset, which we can later on feed to supervised algorithms."
      ],
      "metadata": {
        "id": "sEs6KNnC8fOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know, only 0.1% of real-world data has a label of each item (imagine an image and, next to it, a list of all the animals represented). This is what we call *annotated data*. However, annotation is often:\n",
        "\n",
        "- Expensive\n",
        "- Difficult to standardize\n",
        "- Difficult to scale\n",
        "- Sensitive to design changes\n",
        "- Hard to do right\n",
        "\n"
      ],
      "metadata": {
        "id": "0sftvEbc8yVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.dropbox.com/scl/fi/g5lkwhrbsod6xps2b9fus/unlabeled-data-car.jpg?rlkey=rqe4ge1ctchi5z28epalgusdo&raw=1\"  align=\"center\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pIVzG82494HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples of non-annotated data include:\n",
        "\n",
        "- Chats\n",
        "- Audio\n",
        "- Medical histories\n",
        "\n",
        "So what can we do in that case? One of the common algorithms to attack this problem is _clustering_.\n",
        "\n",
        "Clustering is a set of algorithms that find intrinsic associations in our data to end up with something like this:\n",
        "\n",
        "![Clustering example](https://raw.githubusercontent.com/axel-sirota/getting-started-unsupervised-learning/master/clustering-example.png)\n",
        "\n",
        "There are two fundamental techniques for this: *hierarchical* and *non-hierarchical* clustering."
      ],
      "metadata": {
        "id": "RpqssLLd-C0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchical Clustering"
      ],
      "metadata": {
        "id": "pDnjgea0-JIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Hierarchical clustering, which is also called agglomerative, we add elements in a nested way. For example:\n",
        "\n",
        "![Hierarchical clustering example](https://raw.githubusercontent.com/axel-sirota/getting-started-unsupervised-learning/master/hierarchiecal-clustering.png)\n",
        "\n",
        "To do this, we do the following:\n",
        "\n",
        "1. Start first cluster with the two elements closest to each other\n",
        "2. Calculate the distance between this cluster and the other elements\n",
        "3. Create a new cluster or append to existing one, based on which have the least distance (we append, by treating the new cluster as another element, if it is closest to the element)\n",
        "4. Continue\n",
        "\n",
        "Easy right?\n",
        "\n",
        "Let's do it in Python! First we will import all the necessary packages:"
      ],
      "metadata": {
        "id": "BUqBVPh5-OWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "2iHw61vt8ejx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now load the dataset, which is trivial in scikit-learn given its dataset loading abilities, and fit the hierarchical clustering model:"
      ],
      "metadata": {
        "id": "YqZIYYpJ-VV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "model = AgglomerativeClustering(n_clusters=3, compute_distances=True)\n",
        "model.fit_predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVBhqTMi-W78",
        "outputId": "98c8281a-5271-4015-b1f5-8531bf100b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
              "       2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n",
              "       2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is *THAT* easy! Let's see how well we performed:\n"
      ],
      "metadata": {
        "id": "MLLivXZY_S1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'As expected, the number of clusters predicted by the models are {model.n_clusters_}')\n",
        "\n",
        "predicted_labels = model.labels_\n",
        "corrected_predicted_labels = np.where((predicted_labels==0)|(predicted_labels==1), predicted_labels^1, predicted_labels) # Hierarchiecal starts labels with 1 and the dataset with 0\n",
        "real_labels = iris.target\n",
        "equal = 0\n",
        "for predicted_label, real_label in zip(corrected_predicted_labels, real_labels):\n",
        "    if  predicted_label == real_label:\n",
        "        equal += 1\n",
        "\n",
        "\n",
        "print(f'Accuracy of hierarchiecal clustering with clusters specified is {100*equal / len(corrected_predicted_labels)} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMKeWgZj_REk",
        "outputId": "3c249f1f-13b6-46d0-d058-38914c81b05a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As expected, the number of clusters predicted by the models are 3\n",
            "Accuracy of hierarchiecal clustering with clusters specified is 89.33333333333333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not so bad for a split second operation, right?! What this will do is:\n",
        "\n",
        "1. Get a random flower\n",
        "2. Calculate the distance of it against all other elements\n",
        "3. Create a \"cluster\" of size 2 with the shortest sample\n",
        "4. Iterate until all elements are in a cluster\n",
        "\n",
        "What if we wanted to plot this? This would be as easy as:\n"
      ],
      "metadata": {
        "id": "TFgHYogX_a8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(model, truncate_mode='level', p=3)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "BazaMRra_WvK",
        "outputId": "16b7dc4a-55f8-425f-be64-a13863cdcbbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course you may point out that it's easy when you know *ahead of time* that there are three clusters! And you would be right, but this example is just for you to get your feet wet and start learning.\n",
        "\n",
        "There are multiple ways of optimizing the number of clusters hyperparameter, but that is out of scope for this introduction. If you're curious, I recommend checking out [_Hands-On Unsupervised Learning Using Python_](https://learning.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/) by Ankur A. Patel (O'Reilly).\n",
        "\n",
        "For know, lets move on to non-hierarchical clustering and its most prominent example: _k_-means!"
      ],
      "metadata": {
        "id": "oIMMaJ0eAQfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-Hierarchiechal Clustering"
      ],
      "metadata": {
        "id": "ZfMWB6iAAUiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_k_-means is the most famous clustering algorithm of all, and for a good reason:\n",
        "\n",
        "- It is stable\n",
        "- It scales\n",
        "- It is reliable\n",
        "- It is well studied\n",
        "\n",
        "In a nutshell, the objective of _k_-means is to find inherent clusters using the following recipe:\n",
        "\n",
        "1. Start with _k_ randomly chosen landmarks for starting our clusters.\n",
        "1. Assign every element to a cluster based on minimum distance (which can be anything we chose!).\n",
        "1. Now that there are _k_ clusters, optimize them.\n",
        "1. Calculate the centers of each cluster. This is done by averaging them out. These are the new landmarks for the next iteration.\n",
        "1. Continue until there is no reassignment in step 2.\n",
        "\n",
        "This process is represented by the following:\n",
        "\n",
        "![_k_-means Example](https://raw.githubusercontent.com/axel-sirota/getting-started-unsupervised-learning/master/kmeans.png )\n",
        "\n",
        "First to perform _k_-means, we need to perform our imports:"
      ],
      "metadata": {
        "id": "NiGuQiXBAb9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "np.random.seed(42)\n",
        "del predicted_labels\n",
        "del model\n",
        "del predicted_label\n",
        "del corrected_predicted_labels"
      ],
      "metadata": {
        "id": "jetXSoHB_hys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now load the dataset, which is trivial in scikit-learn with its dataset loading abilities, and then fit the _k_-means model:"
      ],
      "metadata": {
        "id": "dtZe2UvTAjDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "model = KMeans(n_clusters=3, init='random', max_iter=2000, tol=10**-7, random_state=42, n_init='auto')\n",
        "model.fit_predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqivnRzXAg2D",
        "outputId": "33e70261-a5ca-4309-f53e-c90f501e87c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
              "       2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n",
              "       2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare it for a second with the clustering method we saw before by checking it's accuracy:"
      ],
      "metadata": {
        "id": "DFIss4mSAqr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = model.labels_\n",
        "where_0 = np.where(predicted_labels == 0)\n",
        "where_1 = np.where(predicted_labels == 1)\n",
        "\n",
        "predicted_labels[where_0] = 1\n",
        "predicted_labels[where_1] = 0\n",
        "real_labels = iris.target\n",
        "equal = 0\n",
        "for predicted_label, real_label in zip(predicted_labels, real_labels):\n",
        "    if  predicted_label == real_label:\n",
        "        equal += 1\n",
        "\n",
        "\n",
        "print(f'Accuracy of _k_-means clustering with clusters specified is {100*equal / len(predicted_labels)} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgPr-tCKAkhA",
        "outputId": "783c0fef-8f94-438b-b102-9c3b0e7e42f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of _k_-means clustering with clusters specified is 89.33333333333333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Figuring out the number of clusters\n",
        "\n",
        "We can see that it doesn't seem like we have much difference! However, there _is_ a difference, because in _k_-means we have an analytical way of analyzing how to optimize it! Basically, we can calculate for each parameter the inertia of the resulting cluster, so we can optimize over it. This way, we can get the optimum number of clusters *without* knowing them ahead of time!\n",
        "\n",
        "Let's try to do the same as before, but with an increasing number of clusters, and then compare the inertias:"
      ],
      "metadata": {
        "id": "ILhF7rOjA2vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = [1,2,3,4,5,6,7,8]\n",
        "inertias = []\n",
        "for cluster in n_clusters:\n",
        "    del model  # So we remove previous references\n",
        "    model = KMeans(n_clusters=cluster, random_state=42, n_init=10)\n",
        "    model.fit(X)\n",
        "    inertias.append(model.inertia_)\n",
        "\n",
        "print(f'The inertias were: {inertias}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8wIn92TAuqs",
        "outputId": "0571cca0-60f0-4a81-b8d9-addc9172aa35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The inertias were: [681.3706, 152.3479517603579, 78.851441426146, 57.22847321428572, 46.446182051282065, 39.03998724608726, 34.46949589883801, 30.1865551948052]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you check closely, the inertias decrease over time. There is a point, however, where the decrease isn't as steep. This means that we aren't gaining too much information by adding more clusters.\n",
        "\n",
        "The *Elbow method* dictates that the point where the decrease in inertia starts to be \"low\" is the optimal number of clusters. So for our example:"
      ],
      "metadata": {
        "id": "S-MK5wbQBcPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The differences in each step are: {[(i-j) for i, j in zip(inertias[:-1], inertias[1:])]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgTkx8dpBc2v",
        "outputId": "73ed9885-d916-48cd-d711-6fbf21fecde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The differences in each step are: [529.0226482396421, 73.49651033421189, 21.622968211860275, 10.782291163003656, 7.406194805194808, 4.570491347249245, 4.282940704032811]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that going from one cluster to two reduced the inertia by 77%, then we reduced it by another 48% but later on it only reduced by a bit (20 or less); so, we could say the optimal number of clusters is three. What we do then is a hierarchical clustering and confirm this in a dendrogram, as we did before.\n",
        "\n",
        "However, _k_-means is not always the best algorithm since it expects clusters to be globular and of equal shape. Let's quickly analyze this in the next step!"
      ],
      "metadata": {
        "id": "IYqn0GB8BkY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBScan"
      ],
      "metadata": {
        "id": "TIyy7GDEBpdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you find that _k_-means is not working for you, it might be because your data is not equivariant or globular; if so, there are other alternatives! One famous algorithm to revisit is *DB Scan*.\n",
        "\n",
        "The idea behind DB Scan is to not form clusters by assigning all members and later on reduce a metric (inertia in _k_-means), but to check a cluster by the *density of its members*, characterized by the minimum number of elements needed to perform a cluster and the radius of the search space.\n",
        "\n",
        "This way, DB Scan \"scans\" the whole map looking for neighbors and assigns in case there are sufficient, and sufficiently close! Check out this [video][dbscan] for an example.\n",
        "\n",
        "[dbscan]:  http://primo.ai/index.php?title=Density-Based_Spatial_Clustering_of_Applications_with_Noise_(DBSCAN) \"DB Scan Example\"\n",
        "\n",
        "So to get a handle on this in code, first we do our imports:"
      ],
      "metadata": {
        "id": "r4UxF_leBr8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "h8OyN4bHBfhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later on we might create artificial data, but for this we use the super useful `make_blobs` method from scikit-learn:"
      ],
      "metadata": {
        "id": "xl2p7fcIBy-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centers = [[1, 1], [-1, -1], [1, -1]]\n",
        "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
        "                            random_state=0)\n",
        "\n",
        "X = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "WJEGOzd7BxX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll perform the scan, setting the ratio as 0.3 and the minimum elements to create a cluster as 10:"
      ],
      "metadata": {
        "id": "80_cC2A1B2DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
        "labels = db.labels_\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)"
      ],
      "metadata": {
        "id": "0NR8GqOXB0ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's see how it goes! Run the following:\n"
      ],
      "metadata": {
        "id": "5wVCZsD8B7hJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Estimated number of clusters: %d' % n_clusters_)\n",
        "print('Estimated number of noise points: %d' % n_noise_)\n",
        "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
        "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
        "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
        "print(\"Adjusted Rand Index: %0.3f\"\n",
        "      % metrics.adjusted_rand_score(labels_true, labels))\n",
        "print(\"Adjusted Mutual Information: %0.3f\"\n",
        "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
        "print(\"Silhouette Coefficient: %0.3f\"\n",
        "      % metrics.silhouette_score(X, labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BcwUqytB5-O",
        "outputId": "f7450761-4789-418c-fb96-3d5d5a92752a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated number of clusters: 3\n",
            "Estimated number of noise points: 18\n",
            "Homogeneity: 0.953\n",
            "Completeness: 0.883\n",
            "V-measure: 0.917\n",
            "Adjusted Rand Index: 0.952\n",
            "Adjusted Mutual Information: 0.916\n",
            "Silhouette Coefficient: 0.626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not too shabby! DB Scan automatically discovered the three clusters and also removed the 18 outliers, setting them as not being in any cluster!\n",
        "\n",
        "DB Scan is a bit slower than _k_-means and does not parallelize as the latter does, but it is extremely good at finding the \"correct clusters.\" Using the three tools we have seen, we have a perfect toolkit to find the correct number of tentative labels for our data!\n"
      ],
      "metadata": {
        "id": "bHN84zZLCB5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To recap, we did the following:\n",
        "\n",
        "- Learned about unsupervised learning and the types of clustering\n",
        "- Performed a hierarchical clustering, finding out how to visually represent our clusters\n",
        "- Migrated into non hierarchical clustering checking out the most famous algorithm of all, _k_-means, and look at its assumptions and how to find out the number of clusters to set\n",
        "- Finally, we briefly explored DB Scan to analyze density based clustering and how it helps us find, intrinsically, any outliers and the correct number of clusters\n"
      ],
      "metadata": {
        "id": "Q2pnyqh2CI3P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gHsXCm1qB-Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF9_ZRWkUkRw"
      },
      "source": [
        "# Now you do it\n",
        "<img src=\"https://www.dropbox.com/scl/fi/qt7g1wgsnpne43cfwumu0/hands_on.jpg?rlkey=q1zyeuoiuvofnzux4iylfo6ax&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "9MURnOjUgLeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su8q2OiMUkRw"
      },
      "source": [
        "he dataset is one on \"churn\" in cell phone plans. It has information on the usage of the phones by different account holders and whether or not they churned or not.\n",
        "\n",
        "Our goal is to predict whether a user will churn or not based on the other features.\n",
        "\n",
        "<img src=\"https://www.dropbox.com/scl/fi/r0di6ju7bm2pskg5nqd0n/churn.png?rlkey=xclo5ytlre63kb6o31sjub956&raw=1\"  align=\"center\"/>\n",
        "\n",
        "### Use these parameters for testing\n",
        "\n",
        "> random_state = 99\n",
        "\n",
        "> test_size = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vg1_xgrWhMvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile get_capstone_data.sh\n",
        "if [ ! -f churn_missing.csv ]; then\n",
        "  wget -O churn_missing.csv https://www.dropbox.com/scl/fi/r8ctv87bqnb42qjmoyqzy/churn_missing.csv?rlkey=30egvb2y4rpsicz3n0t57w6bd&dl=0\n",
        "fi"
      ],
      "metadata": {
        "id": "leIy-YWOhM7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash get_capstone_data.sh"
      ],
      "metadata": {
        "id": "oG3LqY6_hM7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rPLlRAkUkRw"
      },
      "outputs": [],
      "source": [
        "churn = pd.read_csv('./churn_missing.csv')\n",
        "churn.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKVoCkCPUkRx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VK1ibhKLUkRW",
        "KSob7NpKVP3i",
        "AUyePxrQUkRe",
        "c-oyHgSKUkRk",
        "-_2bFtOmUkRr",
        "Umb7u9dtUkRs",
        "sm--rNNocZm8"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}